{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# General Imposters: Lexico-Grammatical Style\n",
    "\n",
    "The 'General Imposters' method for authorship attribution is a bootstrap-based ensemble classifier that is one of the state-of-the-art approaches. It essentially provides a bootstrap likelihood in answer to this precise question: 'is this document _more similar_ to the style of a _candidate author_ than to any of the _distractor authors_ (imposters)'. It is useful in that the classifier is allowed to express no opinion, usually taken to mean 'the true author is none of the above'--this overcomes a limitation of many categorical machine-learning classifiers, which are obliged to suggest a 'best match' author.\n",
    "\n",
    "In general, this follows the methods and updates the code from this paper:\n",
    "\n",
    "`Kestemont, M., Stover, J., Koppel, M., Karsdorp, F., & Daelemans, W. (2016). Authenticating the writings of Julius Caesar. Expert Systems with Applications, 63, 86-96.`\n",
    "\n",
    "Github for that code is at: https://github.com/mikekestemont/ruzicka\n",
    "My (many) changes are at: https://github.com/bnagy/ruzicka\n",
    "\n",
    "The Kestemont code in turn is based on:\n",
    "\n",
    "`Koppel, M., & Winter, Y. (2014). Determining if two documents are written by the same author. Journal of the Association for Information Science and Technology, 65(1), 178-187.`\n",
    "\n",
    "But I have implemented _some_ of the additional ideas (particularly ranked scoring) from:\n",
    "\n",
    "`Potha, N., & Stamatatos, E. (2017). An improved impostors method for authorship verification. In CLEF 2017, Dublin, Ireland, September 11–14, 2017, Proceedings 8 (pp. 138-144)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from ruzicka.Order2Verifier import Order2Verifier\n",
    "from ruzicka import utilities\n",
    "from ruzicka.score_shifting import ScoreShifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "See [this notebook](build_corpus.ipynb) for corpus creation details. I use Augustan 'short elegy' as elsewhere, but no poem that is less than twenty lines. In addition, I include 200 samples with length $\\sim N(80,20)$ drawn from assorted works of epic hexameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elegy_vecs = pd.read_csv(\"elegy_corpus.csv\", index_col=0)\n",
    "elegy_corpus = elegy_vecs[elegy_vecs.LEN >= 20]\n",
    "hexameter_vecs = pd.read_csv(\"non_elegy_corpus.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Work</th>\n",
       "      <th>Poem</th>\n",
       "      <th>LEN</th>\n",
       "      <th>Chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 1</td>\n",
       "      <td>116</td>\n",
       "      <td>hank tua penelope lento tibi mittit ulikse\\nni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 2</td>\n",
       "      <td>148</td>\n",
       "      <td>hospita demopoon tua te rodopeia pyllis\\nultra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 3</td>\n",
       "      <td>154</td>\n",
       "      <td>kwam legis a rapta briseide littera wenit\\nwik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 4</td>\n",
       "      <td>176</td>\n",
       "      <td>kwam nisi tu dederis karitura_st ipsa salutem\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 5</td>\n",
       "      <td>158</td>\n",
       "      <td>perlegis an konjunks prohibet nowa perlege non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>V.Flaccus</td>\n",
       "      <td>195-Argonautica</td>\n",
       "      <td>195-Argonautica</td>\n",
       "      <td>62</td>\n",
       "      <td>eumenidumkwe komae non tristis ab aetere gorgo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>Lucretius</td>\n",
       "      <td>196-DRN</td>\n",
       "      <td>196-DRN</td>\n",
       "      <td>92</td>\n",
       "      <td>kernere koeperunt kontendere _satkwe parare\\nn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>Horace</td>\n",
       "      <td>197-Hor.</td>\n",
       "      <td>197-Hor. Sat.</td>\n",
       "      <td>72</td>\n",
       "      <td>kwod plakui tibi kwi turpi sekernis honestum\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>Vergil</td>\n",
       "      <td>198-Aeneid</td>\n",
       "      <td>198-Aeneid</td>\n",
       "      <td>79</td>\n",
       "      <td>insinjem gemmis tum fumida lumine fulwo\\ninwol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>Vergil</td>\n",
       "      <td>199-Georgics</td>\n",
       "      <td>199-Georgics</td>\n",
       "      <td>76</td>\n",
       "      <td>saepius et tektis penetralibus ekstulit owa\\na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>488 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Author             Work             Poem  LEN  \\\n",
       "0         Ovid              Ep.            Ep. 1  116   \n",
       "1         Ovid              Ep.            Ep. 2  148   \n",
       "2         Ovid              Ep.            Ep. 3  154   \n",
       "3         Ovid              Ep.            Ep. 4  176   \n",
       "4         Ovid              Ep.            Ep. 5  158   \n",
       "..         ...              ...              ...  ...   \n",
       "483  V.Flaccus  195-Argonautica  195-Argonautica   62   \n",
       "484  Lucretius          196-DRN          196-DRN   92   \n",
       "485     Horace         197-Hor.    197-Hor. Sat.   72   \n",
       "486     Vergil       198-Aeneid       198-Aeneid   79   \n",
       "487     Vergil     199-Georgics     199-Georgics   76   \n",
       "\n",
       "                                                 Chunk  \n",
       "0    hank tua penelope lento tibi mittit ulikse\\nni...  \n",
       "1    hospita demopoon tua te rodopeia pyllis\\nultra...  \n",
       "2    kwam legis a rapta briseide littera wenit\\nwik...  \n",
       "3    kwam nisi tu dederis karitura_st ipsa salutem\\...  \n",
       "4    perlegis an konjunks prohibet nowa perlege non...  \n",
       "..                                                 ...  \n",
       "483  eumenidumkwe komae non tristis ab aetere gorgo...  \n",
       "484  kernere koeperunt kontendere _satkwe parare\\nn...  \n",
       "485  kwod plakui tibi kwi turpi sekernis honestum\\n...  \n",
       "486  insinjem gemmis tum fumida lumine fulwo\\ninwol...  \n",
       "487  saepius et tektis penetralibus ekstulit owa\\na...  \n",
       "\n",
       "[488 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus = pd.concat(\n",
    "    [elegy_corpus[elegy_corpus.Author != \"ps-Ovid\"], hexameter_vecs]\n",
    ").reset_index(drop=True)\n",
    "test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenc = LabelEncoder()\n",
    "labels = lenc.fit_transform(test_corpus.Author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"ruzicka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to logging.DEBUG or higher for less noise\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    handler.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier options\n",
    "\n",
    "verifier_nini = Order2Verifier(\n",
    "    metric=\"nini\", base=\"instance\", nb_bootstrap_iter=500, rnd_prop=0.35\n",
    ")\n",
    "\n",
    "verifier_minmax = Order2Verifier(\n",
    "    metric=\"minmax\", base=\"instance\", nb_bootstrap_iter=500, rnd_prop=0.35\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "I test two methods (which turn out to perform similarly). \n",
    "\n",
    "First, I use _z_-scaling and the MinMax (Ruzička) metric as recommended by the Kestemont paper, but apply it to 2-, 3-, and 4-grams instead of most frequent words (MFW). This is my general approach when dealing with small poetic samples.\n",
    "\n",
    "Second, I use the approach described elsewhere from _Nini, A. (2023). A Theory of Linguistic Individuality for Authorship Analysis (Elements in Forensic Linguistics). Cambridge: Cambridge University Press. doi:10.1017/9781108974851_. Here the vector space is 5000 most frequent 5-grams and the metric is the Pearson Correlation between set-inclusion indicator (binary) vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer options\n",
    "\n",
    "vec_ngrams_std = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        use_idf=False,\n",
    "        norm=\"l2\",\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(2, 4),\n",
    "        max_features=5000,\n",
    "    ),\n",
    "    StandardScaler(\n",
    "        with_mean=False\n",
    "    ),  # never centre frequency data for the minmax metric!\n",
    "    FunctionTransformer(lambda x: x.todense(), accept_sparse=True),\n",
    "    Normalizer(),\n",
    ")\n",
    "\n",
    "vec_5grams = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        use_idf=False,\n",
    "        norm=\"l2\",\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(5, 5),\n",
    "        max_features=5000,\n",
    "    ),\n",
    "    FunctionTransformer(lambda x: x.todense(), accept_sparse=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison / Evaluation\n",
    "\n",
    "In each case we fit a 'score shifter' on a random 20% subsample and then apply that shifting to the GI Verifier. The final metric is C@1 Accuracy:\n",
    "\n",
    "`A. Peñas and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n",
    "        In Proc. of the 49th Annual Meeting of the Association for\n",
    "        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.`\n",
    "\n",
    "This measure is useful because it allows the model to refuse to classify (to say 'I don't know') without unduly penalising it, which helps with regularisation and interpretability.\n",
    "\n",
    "In general, both methods perform very strongly, especially given the small samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/13/2025 02:12:00 [ruzicka:INFO] Fitting the provided score shifter on a 20.0% sample\n",
      "01/13/2025 02:12:01 [ruzicka:INFO] Fitting on 390 documents in instance mode...\n",
      "01/13/2025 02:12:01 [ruzicka:INFO] Running verifier on sub-sample\n",
      "01/13/2025 02:12:01 [ruzicka:INFO] Predicting on 196 documents\n",
      "01/13/2025 02:12:41 [ruzicka:INFO] Actually fitting...\n",
      "01/13/2025 02:12:41 [ruzicka:INFO] p1 for optimal combo: 0.584\n",
      "01/13/2025 02:12:41 [ruzicka:INFO] p2 for optimal combo: 0.788\n",
      "01/13/2025 02:12:41 [ruzicka:INFO] Objective function result for optimal combo: 95.50%\n",
      "01/13/2025 02:12:41 [ruzicka:INFO] Starting benchmark: 10 splits, test size 10%\n",
      "01/13/2025 02:12:42 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:12:42 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:13:03 [ruzicka:INFO] Accuracy: 88.78% AUC: 99.92% c@1: 96.93% AUC x c@1: 96.85%\n",
      "01/13/2025 02:13:04 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:13:04 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:13:25 [ruzicka:INFO] Accuracy: 93.88% AUC: 99.81% c@1: 96.65% AUC x c@1: 96.47%\n",
      "01/13/2025 02:13:26 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:13:26 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:13:47 [ruzicka:INFO] Accuracy: 86.73% AUC: 99.83% c@1: 92.93% AUC x c@1: 92.78%\n",
      "01/13/2025 02:13:47 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:13:47 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:14:09 [ruzicka:INFO] Accuracy: 84.69% AUC: 99.56% c@1: 93.92% AUC x c@1: 93.51%\n",
      "01/13/2025 02:14:09 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:14:09 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:14:30 [ruzicka:INFO] Accuracy: 87.76% AUC: 99.29% c@1: 92.93% AUC x c@1: 92.27%\n",
      "01/13/2025 02:14:31 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:14:31 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:14:52 [ruzicka:INFO] Accuracy: 88.78% AUC: 99.48% c@1: 94.92% AUC x c@1: 94.42%\n",
      "01/13/2025 02:14:52 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:14:52 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:15:13 [ruzicka:INFO] Accuracy: 90.82% AUC: 99.67% c@1: 96.21% AUC x c@1: 95.89%\n",
      "01/13/2025 02:15:14 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:15:14 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:15:35 [ruzicka:INFO] Accuracy: 89.80% AUC: 99.71% c@1: 96.93% AUC x c@1: 96.65%\n",
      "01/13/2025 02:15:35 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:15:35 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:15:57 [ruzicka:INFO] Accuracy: 87.76% AUC: 99.92% c@1: 93.13% AUC x c@1: 93.05%\n",
      "01/13/2025 02:15:58 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:15:58 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:16:18 [ruzicka:INFO] Accuracy: 91.84% AUC: 99.79% c@1: 97.30% AUC x c@1: 97.10%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Splits:  10\n",
      "   Test %:  10%\n",
      " Accuracy:  Mean 89.08%, SD 0.03\n",
      "      C@1:  Mean 95.18%, SD 0.02\n"
     ]
    }
   ],
   "source": [
    "nini_shifter = utilities.fit_shifter(\n",
    "    test_corpus.Chunk,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    vectorizer=vec_5grams,\n",
    "    verifier=verifier_nini,\n",
    "    shifter=ScoreShifter(min_spread=0.2),\n",
    ")\n",
    "aa, cc = utilities.benchmark_imposters(\n",
    "    test_corpus.Chunk, labels, sss, vec_5grams, verifier_nini, nini_shifter\n",
    ")\n",
    "print()\n",
    "print(f\"{'Splits: ':>11} {sss.n_splits}\")\n",
    "print(f\"{'Test %: ':>11} {sss.test_size:.0%}\")\n",
    "print(f\"{'Accuracy: ':>11} Mean {np.mean(aa):.2%}, SD {np.std(aa):.2f}\")\n",
    "print(f\"{'C@1: ':>11} Mean {np.mean(cc):.2%}, SD {np.std(cc):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/13/2025 02:16:18 [ruzicka:INFO] Fitting the provided score shifter on a 20.0% sample\n",
      "01/13/2025 02:16:19 [ruzicka:INFO] Fitting on 390 documents in instance mode...\n",
      "01/13/2025 02:16:20 [ruzicka:INFO] Running verifier on sub-sample\n",
      "01/13/2025 02:16:20 [ruzicka:INFO] Predicting on 196 documents\n",
      "01/13/2025 02:16:56 [ruzicka:INFO] Actually fitting...\n",
      "01/13/2025 02:16:57 [ruzicka:INFO] p1 for optimal combo: 0.590\n",
      "01/13/2025 02:16:57 [ruzicka:INFO] p2 for optimal combo: 0.794\n",
      "01/13/2025 02:16:57 [ruzicka:INFO] Objective function result for optimal combo: 93.90%\n",
      "01/13/2025 02:16:57 [ruzicka:INFO] Starting benchmark: 10 splits, test size 10%\n",
      "01/13/2025 02:16:58 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:16:58 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:17:18 [ruzicka:INFO] Accuracy: 86.73% AUC: 99.40% c@1: 94.46% AUC x c@1: 93.89%\n",
      "01/13/2025 02:17:19 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:17:19 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:17:39 [ruzicka:INFO] Accuracy: 90.82% AUC: 99.92% c@1: 96.38% AUC x c@1: 96.30%\n",
      "01/13/2025 02:17:40 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:17:40 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:18:00 [ruzicka:INFO] Accuracy: 94.90% AUC: 99.67% c@1: 97.59% AUC x c@1: 97.27%\n",
      "01/13/2025 02:18:01 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:18:01 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:18:20 [ruzicka:INFO] Accuracy: 90.82% AUC: 99.83% c@1: 97.30% AUC x c@1: 97.14%\n",
      "01/13/2025 02:18:21 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:18:22 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:18:41 [ruzicka:INFO] Accuracy: 91.84% AUC: 99.21% c@1: 93.46% AUC x c@1: 92.72%\n",
      "01/13/2025 02:18:42 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:18:42 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:19:02 [ruzicka:INFO] Accuracy: 89.80% AUC: 97.58% c@1: 95.12% AUC x c@1: 92.82%\n",
      "01/13/2025 02:19:03 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:19:03 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:19:22 [ruzicka:INFO] Accuracy: 91.84% AUC: 98.90% c@1: 96.02% AUC x c@1: 94.96%\n",
      "01/13/2025 02:19:23 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:19:23 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:19:43 [ruzicka:INFO] Accuracy: 91.84% AUC: 100.00% c@1: 96.52% AUC x c@1: 96.52%\n",
      "01/13/2025 02:19:44 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:19:44 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:20:04 [ruzicka:INFO] Accuracy: 89.80% AUC: 99.96% c@1: 96.21% AUC x c@1: 96.17%\n",
      "01/13/2025 02:20:05 [ruzicka:INFO] Fitting on 439 documents in instance mode...\n",
      "01/13/2025 02:20:06 [ruzicka:INFO] Predicting on 98 documents\n",
      "01/13/2025 02:20:27 [ruzicka:INFO] Accuracy: 89.80% AUC: 99.35% c@1: 94.21% AUC x c@1: 93.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Splits:  10\n",
      "   Test %:  10%\n",
      " Accuracy:  Mean 90.82%, SD 0.02\n",
      "      C@1:  Mean 95.73%, SD 0.01\n"
     ]
    }
   ],
   "source": [
    "ngram_shifter = utilities.fit_shifter(\n",
    "    test_corpus.Chunk,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    vectorizer=vec_ngrams_std,\n",
    "    verifier=verifier_minmax,\n",
    "    shifter=ScoreShifter(min_spread=0.2),\n",
    ")\n",
    "aa, cc = utilities.benchmark_imposters(\n",
    "    test_corpus.Chunk, labels, sss, vec_ngrams_std, verifier_minmax, ngram_shifter\n",
    ")\n",
    "print()\n",
    "print(f\"{'Splits: ':>11} {sss.n_splits}\")\n",
    "print(f\"{'Test %: ':>11} {sss.test_size:.0%}\")\n",
    "print(f\"{'Accuracy: ':>11} Mean {np.mean(aa):.2%}, SD {np.std(aa):.2f}\")\n",
    "print(f\"{'C@1: ':>11} Mean {np.mean(cc):.2%}, SD {np.std(cc):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually apply the method\n",
    "\n",
    "Finally, we apply the method to the problem texts. All the texts are classified as certainly closer to Ovidian style than to any other author (1 is '100% confidence'). This suggests that this method, particularly when applied to purely textual input does not have the statistical power to distinguish between Ovidian imitation (_Consolatio_) and genuine work (_Ibis_, _Medicamina_). This leaves the situation unclear with respect to the _Nux_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_verifier = Order2Verifier(\n",
    "    metric=\"minmax\", base=\"instance\", nb_bootstrap_iter=1000, rnd_prop=0.35\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/13/2025 02:20:28 [ruzicka:INFO] Fitting on 488 documents in instance mode...\n"
     ]
    }
   ],
   "source": [
    "real_verifier.fit(vec_ngrams_std.fit_transform(test_corpus.Chunk), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Work</th>\n",
       "      <th>Poem</th>\n",
       "      <th>LEN</th>\n",
       "      <th>Chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Nux</td>\n",
       "      <td>Nux</td>\n",
       "      <td>182</td>\n",
       "      <td>nuks ego junkta wiae kum sim sine krimine wita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Medicamina</td>\n",
       "      <td>Medicamina</td>\n",
       "      <td>100</td>\n",
       "      <td>diskite kwae fakiem kommendet kura puellae\\net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Pamphileas</td>\n",
       "      <td>Pamphileas</td>\n",
       "      <td>198</td>\n",
       "      <td>postkwam pampileas rumor perwenit ad aures\\ngl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Consolatio</td>\n",
       "      <td>Consolatio 1</td>\n",
       "      <td>158</td>\n",
       "      <td>wisa diu feliks mater modo dikta neronum\\njam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Consolatio</td>\n",
       "      <td>Consolatio 2</td>\n",
       "      <td>158</td>\n",
       "      <td>at_kwutinam drusi manus alte_ret altera fratri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Consolatio</td>\n",
       "      <td>Consolatio 3</td>\n",
       "      <td>158</td>\n",
       "      <td>kwo raperis laniata komas similiskwe furenti\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Ibis</td>\n",
       "      <td>Ibis 1</td>\n",
       "      <td>64</td>\n",
       "      <td>tempus ad hok lustris bis jam mihi kwinkwe per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Ibis</td>\n",
       "      <td>Ibis 2</td>\n",
       "      <td>200</td>\n",
       "      <td>di maris et terrae kwi_kwis meliora tenetis\\ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Ibis</td>\n",
       "      <td>Ibis 3</td>\n",
       "      <td>200</td>\n",
       "      <td>kwi_kwokulis karuit per kwos male widerat auru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Ibis</td>\n",
       "      <td>Ibis 4</td>\n",
       "      <td>178</td>\n",
       "      <td>aut te dewoweat kertis abdera diebus\\nsaksakwe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Author        Work          Poem  LEN  \\\n",
       "296  ps-Ovid         Nux           Nux  182   \n",
       "297  ps-Ovid  Medicamina    Medicamina  100   \n",
       "298  ps-Ovid  Pamphileas    Pamphileas  198   \n",
       "299  ps-Ovid  Consolatio  Consolatio 1  158   \n",
       "300  ps-Ovid  Consolatio  Consolatio 2  158   \n",
       "301  ps-Ovid  Consolatio  Consolatio 3  158   \n",
       "302  ps-Ovid        Ibis        Ibis 1   64   \n",
       "303  ps-Ovid        Ibis        Ibis 2  200   \n",
       "304  ps-Ovid        Ibis        Ibis 3  200   \n",
       "305  ps-Ovid        Ibis        Ibis 4  178   \n",
       "\n",
       "                                                 Chunk  \n",
       "296  nuks ego junkta wiae kum sim sine krimine wita...  \n",
       "297  diskite kwae fakiem kommendet kura puellae\\net...  \n",
       "298  postkwam pampileas rumor perwenit ad aures\\ngl...  \n",
       "299  wisa diu feliks mater modo dikta neronum\\njam ...  \n",
       "300  at_kwutinam drusi manus alte_ret altera fratri...  \n",
       "301  kwo raperis laniata komas similiskwe furenti\\n...  \n",
       "302  tempus ad hok lustris bis jam mihi kwinkwe per...  \n",
       "303  di maris et terrae kwi_kwis meliora tenetis\\ni...  \n",
       "304  kwi_kwokulis karuit per kwos male widerat auru...  \n",
       "305  aut te dewoweat kertis abdera diebus\\nsaksakwe...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems = elegy_corpus[elegy_corpus.Author == \"ps-Ovid\"]\n",
    "problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/13/2025 02:20:28 [ruzicka:INFO] Predicting on 10 documents\n"
     ]
    }
   ],
   "source": [
    "t = real_verifier.predict_proba(\n",
    "    vec_ngrams_std.transform(problems.Chunk),\n",
    "    np.array(lenc.transform([\"Ovid\"] * len(problems))),\n",
    "    nb_imposters=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poem</th>\n",
       "      <th>GI %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nux</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medicamina</td>\n",
       "      <td>0.995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pamphileas</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consolatio 1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consolatio 2</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consolatio 3</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ibis 1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ibis 2</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ibis 3</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ibis 4</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Poem   GI %\n",
       "0           Nux  1.000\n",
       "1    Medicamina  0.995\n",
       "2    Pamphileas  0.999\n",
       "3  Consolatio 1  1.000\n",
       "4  Consolatio 2  1.000\n",
       "5  Consolatio 3  1.000\n",
       "6        Ibis 1  1.000\n",
       "7        Ibis 2  1.000\n",
       "8        Ibis 3  1.000\n",
       "9        Ibis 4  1.000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(problems.Poem, t), columns=[\"Poem\", \"GI %\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "208657b13d9d01f546253097cf6f870938d682edd7d0d269d6436fd16db9d0e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
