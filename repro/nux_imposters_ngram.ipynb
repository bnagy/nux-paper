{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# General Imposters: Lexico-Grammatical Style\n",
    "\n",
    "The 'General Imposters' method for authorship attribution is a bootstrap-based ensemble classifier that is one of the state-of-the-art approaches. It essentially provides a bootstrap likelihood in answer to this precise question: 'is this document _more similar_ to the style of a _candidate author_ than to any of the _distractor authors_ (imposters)'. It is useful in that the classifier is allowed to express no opinion, usually taken to mean 'the true author is none of the above'--this overcomes a limitation of many categorical machine-learning classifiers, which are obliged to suggest a 'best match' author.\n",
    "\n",
    "In general, this follows the methods and updates the code from this paper:\n",
    "\n",
    "`Kestemont, M., Stover, J., Koppel, M., Karsdorp, F., & Daelemans, W. (2016). Authenticating the writings of Julius Caesar. Expert Systems with Applications, 63, 86-96.`\n",
    "\n",
    "Github for that code is at: https://github.com/mikekestemont/ruzicka\n",
    "My (many) changes are at: https://github.com/bnagy/ruzicka\n",
    "\n",
    "The Kestemont code in turn is based on:\n",
    "\n",
    "`Koppel, M., & Winter, Y. (2014). Determining if two documents are written by the same author. Journal of the Association for Information Science and Technology, 65(1), 178-187.`\n",
    "\n",
    "But I have implemented _some_ of the additional ideas (particularly ranked scoring) from:\n",
    "\n",
    "`Potha, N., & Stamatatos, E. (2017). An improved impostors method for authorship verification. In CLEF 2017, Dublin, Ireland, September 11–14, 2017, Proceedings 8 (pp. 138-144)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from ruzicka.Order2Verifier import Order2Verifier\n",
    "from ruzicka import utilities\n",
    "from ruzicka.score_shifting import ScoreShifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "See [this notebook](build_corpus.ipynb) for corpus creation details. I use Augustan 'short elegy' as elsewhere, but no poem that is less than twenty lines. In addition, I include 200 samples with length $\\sim N(80,20)$ drawn from assorted works of epic hexameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elegy_vecs = pd.read_csv(\"elegy_corpus.csv\", index_col=0)\n",
    "elegy_corpus = elegy_vecs[elegy_vecs.LEN >= 20]\n",
    "hexameter_vecs = pd.read_csv(\"non_elegy_corpus.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Work</th>\n",
       "      <th>Poem</th>\n",
       "      <th>LEN</th>\n",
       "      <th>Chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 1</td>\n",
       "      <td>116</td>\n",
       "      <td>hank tua penelope lento tibi mittit ulikse\\nni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 2</td>\n",
       "      <td>148</td>\n",
       "      <td>hospita demopoon tua te rodopeia pyllis\\nultra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 3</td>\n",
       "      <td>154</td>\n",
       "      <td>kwam legis a rapta briseide littera wenit\\nwik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 4</td>\n",
       "      <td>176</td>\n",
       "      <td>kwam nisi tu dederis karitura_st ipsa salutem\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ovid</td>\n",
       "      <td>Ep.</td>\n",
       "      <td>Ep. 5</td>\n",
       "      <td>158</td>\n",
       "      <td>perlegis an konjunks prohibet nowa perlege non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>V.Flaccus</td>\n",
       "      <td>195-Argonautica</td>\n",
       "      <td>195-Argonautica</td>\n",
       "      <td>98</td>\n",
       "      <td>si pelopis duros prior hippodamia labores\\neks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>Lucretius</td>\n",
       "      <td>196-DRN</td>\n",
       "      <td>196-DRN</td>\n",
       "      <td>93</td>\n",
       "      <td>dekiderunt kwo_kwet in talis wenere meatus\\nkw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>Horace</td>\n",
       "      <td>197-Hor.</td>\n",
       "      <td>197-Hor. Sat.</td>\n",
       "      <td>94</td>\n",
       "      <td>eksirem plures kalones atkwe kaballi\\npaskendi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>Vergil</td>\n",
       "      <td>198-Aeneid</td>\n",
       "      <td>198-Aeneid</td>\n",
       "      <td>106</td>\n",
       "      <td>in lukem genito_ramyko dedit et fake praenjas\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>Vergil</td>\n",
       "      <td>199-Georgics</td>\n",
       "      <td>199-Georgics</td>\n",
       "      <td>31</td>\n",
       "      <td>tum pater omnipotens fekundis imbribus aeter\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Author             Work             Poem  LEN  \\\n",
       "0         Ovid              Ep.            Ep. 1  116   \n",
       "1         Ovid              Ep.            Ep. 2  148   \n",
       "2         Ovid              Ep.            Ep. 3  154   \n",
       "3         Ovid              Ep.            Ep. 4  176   \n",
       "4         Ovid              Ep.            Ep. 5  158   \n",
       "..         ...              ...              ...  ...   \n",
       "465  V.Flaccus  195-Argonautica  195-Argonautica   98   \n",
       "466  Lucretius          196-DRN          196-DRN   93   \n",
       "467     Horace         197-Hor.    197-Hor. Sat.   94   \n",
       "468     Vergil       198-Aeneid       198-Aeneid  106   \n",
       "469     Vergil     199-Georgics     199-Georgics   31   \n",
       "\n",
       "                                                 Chunk  \n",
       "0    hank tua penelope lento tibi mittit ulikse\\nni...  \n",
       "1    hospita demopoon tua te rodopeia pyllis\\nultra...  \n",
       "2    kwam legis a rapta briseide littera wenit\\nwik...  \n",
       "3    kwam nisi tu dederis karitura_st ipsa salutem\\...  \n",
       "4    perlegis an konjunks prohibet nowa perlege non...  \n",
       "..                                                 ...  \n",
       "465  si pelopis duros prior hippodamia labores\\neks...  \n",
       "466  dekiderunt kwo_kwet in talis wenere meatus\\nkw...  \n",
       "467  eksirem plures kalones atkwe kaballi\\npaskendi...  \n",
       "468  in lukem genito_ramyko dedit et fake praenjas\\...  \n",
       "469  tum pater omnipotens fekundis imbribus aeter\\n...  \n",
       "\n",
       "[470 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus = pd.concat(\n",
    "    [elegy_corpus[elegy_corpus.Author != \"ps-Ovid\"], hexameter_vecs]\n",
    ").reset_index(drop=True)\n",
    "test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenc = LabelEncoder()\n",
    "labels = lenc.fit_transform(test_corpus.Author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"ruzicka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to logging.DEBUG or higher for less noise\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    handler.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifier options\n",
    "\n",
    "verifier_nini = Order2Verifier(\n",
    "    metric=\"nini\", base=\"instance\", nb_bootstrap_iter=500, rnd_prop=0.35\n",
    ")\n",
    "\n",
    "verifier_minmax = Order2Verifier(\n",
    "    metric=\"minmax\", base=\"instance\", nb_bootstrap_iter=500, rnd_prop=0.35\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "I test two methods (which turn out to perform similarly). \n",
    "\n",
    "First, I use _z_-scaling and the MinMax (Ruzička) metric as recommended by the Kestemont paper, but apply it to 2-, 3-, and 4-grams instead of most frequent words (MFW). This is my general approach when dealing with small poetic samples.\n",
    "\n",
    "Second, I use the approach described elsewhere from _Nini, A. (2023). A Theory of Linguistic Individuality for Authorship Analysis (Elements in Forensic Linguistics). Cambridge: Cambridge University Press. doi:10.1017/9781108974851_. Here the vector space is 5000 most frequent 5-grams and the metric is the Pearson Correlation between set-inclusion indicator (binary) vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer options\n",
    "\n",
    "vec_ngrams_std = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        use_idf=False,\n",
    "        norm=\"l2\",\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(2, 4),\n",
    "        max_features=5000,\n",
    "    ),\n",
    "    StandardScaler(\n",
    "        with_mean=False\n",
    "    ),  # never centre frequency data for the minmax metric!\n",
    "    FunctionTransformer(lambda x: x.todense(), accept_sparse=True),\n",
    "    Normalizer(),\n",
    ")\n",
    "\n",
    "vec_5grams = make_pipeline(\n",
    "    TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        use_idf=False,\n",
    "        norm=\"l2\",\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(5, 5),\n",
    "        max_features=5000,\n",
    "    ),\n",
    "    FunctionTransformer(lambda x: x.todense(), accept_sparse=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison / Evaluation\n",
    "\n",
    "In each case we fit a 'score shifter' on a random 20% subsample and then apply that shifting to the GI Verifier. The final metric is C@1 Accuracy:\n",
    "\n",
    "`A. Peñas and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n",
    "        In Proc. of the 49th Annual Meeting of the Association for\n",
    "        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.`\n",
    "\n",
    "This measure is useful because it allows the model to refuse to classify (to say 'I don't know') without unduly penalising it, which helps with regularisation and interpretability.\n",
    "\n",
    "In general, both methods perform very strongly, especially given the small samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/12/2023 02:24:23 [ruzicka:INFO] Fitting the provided score shifter on a 20.0% sample\n",
      "09/12/2023 02:24:23 [ruzicka:INFO] Fitting on 376 documents in instance mode...\n",
      "09/12/2023 02:24:23 [ruzicka:INFO] Running verifier on sub-sample\n",
      "09/12/2023 02:24:23 [ruzicka:INFO] Predicting on 188 documents\n",
      "09/12/2023 02:25:15 [ruzicka:INFO] Actually fitting...\n",
      "09/12/2023 02:25:19 [ruzicka:INFO] p1 for optimal combo: 0.590\n",
      "09/12/2023 02:25:19 [ruzicka:INFO] p2 for optimal combo: 0.794\n",
      "09/12/2023 02:25:19 [ruzicka:INFO] AUC for optimal combo: 99.75%\n",
      "09/12/2023 02:25:19 [ruzicka:INFO] c@1 for optimal combo: 95.31%\n",
      "09/12/2023 02:25:19 [ruzicka:INFO] Starting benchmark: 10 splits, test size 10%\n",
      "09/12/2023 02:25:20 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:25:20 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:25:47 [ruzicka:INFO] Accuracy: 89.36% AUC: 99.86% c@1: 96.02% AUC x c@1: 95.89%\n",
      "09/12/2023 02:25:47 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:25:47 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:26:15 [ruzicka:INFO] Accuracy: 91.49% AUC: 99.73% c@1: 95.24% AUC x c@1: 94.98%\n",
      "09/12/2023 02:26:16 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:26:16 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:26:44 [ruzicka:INFO] Accuracy: 87.23% AUC: 99.05% c@1: 92.59% AUC x c@1: 91.71%\n",
      "09/12/2023 02:26:45 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:26:45 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:27:12 [ruzicka:INFO] Accuracy: 88.30% AUC: 99.66% c@1: 94.66% AUC x c@1: 94.34%\n",
      "09/12/2023 02:27:13 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:27:13 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:27:41 [ruzicka:INFO] Accuracy: 92.55% AUC: 99.91% c@1: 97.48% AUC x c@1: 97.39%\n",
      "09/12/2023 02:27:41 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:27:41 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:28:08 [ruzicka:INFO] Accuracy: 90.43% AUC: 99.73% c@1: 96.02% AUC x c@1: 95.76%\n",
      "09/12/2023 02:28:09 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:28:09 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:28:36 [ruzicka:INFO] Accuracy: 86.17% AUC: 99.64% c@1: 92.59% AUC x c@1: 92.25%\n",
      "09/12/2023 02:28:36 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:28:36 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:29:03 [ruzicka:INFO] Accuracy: 90.43% AUC: 99.73% c@1: 96.02% AUC x c@1: 95.76%\n",
      "09/12/2023 02:29:04 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:29:04 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:29:30 [ruzicka:INFO] Accuracy: 93.62% AUC: 99.21% c@1: 96.49% AUC x c@1: 95.73%\n",
      "09/12/2023 02:29:31 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:29:31 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:29:58 [ruzicka:INFO] Accuracy: 86.17% AUC: 99.77% c@1: 93.50% AUC x c@1: 93.29%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Splits:  10\n",
      "   Test %:  10%\n",
      " Accuracy:  Mean 89.57%, SD 0.02\n",
      "      C@1:  Mean 95.06%, SD 0.02\n"
     ]
    }
   ],
   "source": [
    "nini_shifter = utilities.fit_shifter(\n",
    "    test_corpus.Chunk,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    vectorizer=vec_5grams,\n",
    "    verifier=verifier_nini,\n",
    "    shifter=ScoreShifter(min_spread=0.2),\n",
    ")\n",
    "aa, cc = utilities.benchmark_imposters(\n",
    "    test_corpus.Chunk, labels, sss, vec_5grams, verifier_nini, nini_shifter\n",
    ")\n",
    "print()\n",
    "print(f\"{'Splits: ':>11} {sss.n_splits}\")\n",
    "print(f\"{'Test %: ':>11} {sss.test_size:.0%}\")\n",
    "print(f\"{'Accuracy: ':>11} Mean {np.mean(aa):.2%}, SD {np.std(aa):.2f}\")\n",
    "print(f\"{'C@1: ':>11} Mean {np.mean(cc):.2%}, SD {np.std(cc):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/12/2023 02:29:58 [ruzicka:INFO] Fitting the provided score shifter on a 20.0% sample\n",
      "09/12/2023 02:29:59 [ruzicka:INFO] Fitting on 376 documents in instance mode...\n",
      "09/12/2023 02:29:59 [ruzicka:INFO] Running verifier on sub-sample\n",
      "09/12/2023 02:29:59 [ruzicka:INFO] Predicting on 188 documents\n",
      "09/12/2023 02:30:44 [ruzicka:INFO] Actually fitting...\n",
      "09/12/2023 02:30:49 [ruzicka:INFO] p1 for optimal combo: 0.590\n",
      "09/12/2023 02:30:49 [ruzicka:INFO] p2 for optimal combo: 0.794\n",
      "09/12/2023 02:30:49 [ruzicka:INFO] AUC for optimal combo: 99.45%\n",
      "09/12/2023 02:30:49 [ruzicka:INFO] c@1 for optimal combo: 98.30%\n",
      "09/12/2023 02:30:49 [ruzicka:INFO] Starting benchmark: 10 splits, test size 10%\n",
      "09/12/2023 02:30:50 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:30:50 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:31:14 [ruzicka:INFO] Accuracy: 87.23% AUC: 99.52% c@1: 95.34% AUC x c@1: 94.88%\n",
      "09/12/2023 02:31:16 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:31:16 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:31:40 [ruzicka:INFO] Accuracy: 89.36% AUC: 99.12% c@1: 93.93% AUC x c@1: 93.10%\n",
      "09/12/2023 02:31:41 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:31:41 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:32:05 [ruzicka:INFO] Accuracy: 87.23% AUC: 96.70% c@1: 94.42% AUC x c@1: 91.30%\n",
      "09/12/2023 02:32:06 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:32:06 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:32:30 [ruzicka:INFO] Accuracy: 89.36% AUC: 99.23% c@1: 96.02% AUC x c@1: 95.28%\n",
      "09/12/2023 02:32:31 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:32:31 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:32:55 [ruzicka:INFO] Accuracy: 88.30% AUC: 99.39% c@1: 92.80% AUC x c@1: 92.24%\n",
      "09/12/2023 02:32:56 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:32:56 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:33:22 [ruzicka:INFO] Accuracy: 85.11% AUC: 99.77% c@1: 91.44% AUC x c@1: 91.24%\n",
      "09/12/2023 02:33:23 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:33:23 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:33:48 [ruzicka:INFO] Accuracy: 88.30% AUC: 98.26% c@1: 95.59% AUC x c@1: 93.92%\n",
      "09/12/2023 02:33:49 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:33:49 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:34:14 [ruzicka:INFO] Accuracy: 89.36% AUC: 100.00% c@1: 95.07% AUC x c@1: 95.07%\n",
      "09/12/2023 02:34:15 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:34:15 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:34:39 [ruzicka:INFO] Accuracy: 90.43% AUC: 99.68% c@1: 96.20% AUC x c@1: 95.89%\n",
      "09/12/2023 02:34:40 [ruzicka:INFO] Fitting on 423 documents in instance mode...\n",
      "09/12/2023 02:34:41 [ruzicka:INFO] Predicting on 94 documents\n",
      "09/12/2023 02:35:05 [ruzicka:INFO] Accuracy: 90.43% AUC: 99.41% c@1: 94.11% AUC x c@1: 93.56%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Splits:  10\n",
      "   Test %:  10%\n",
      " Accuracy:  Mean 88.51%, SD 0.02\n",
      "      C@1:  Mean 94.49%, SD 0.01\n"
     ]
    }
   ],
   "source": [
    "ngram_shifter = utilities.fit_shifter(\n",
    "    test_corpus.Chunk,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    vectorizer=vec_ngrams_std,\n",
    "    verifier=verifier_minmax,\n",
    "    shifter=ScoreShifter(min_spread=0.2),\n",
    ")\n",
    "aa, cc = utilities.benchmark_imposters(\n",
    "    test_corpus.Chunk, labels, sss, vec_ngrams_std, verifier_minmax, ngram_shifter\n",
    ")\n",
    "print()\n",
    "print(f\"{'Splits: ':>11} {sss.n_splits}\")\n",
    "print(f\"{'Test %: ':>11} {sss.test_size:.0%}\")\n",
    "print(f\"{'Accuracy: ':>11} Mean {np.mean(aa):.2%}, SD {np.std(aa):.2f}\")\n",
    "print(f\"{'C@1: ':>11} Mean {np.mean(cc):.2%}, SD {np.std(cc):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually apply the method\n",
    "\n",
    "Finally, we apply the method to the problem texts. All the texts are classified as certainly closer to Ovidian style than to any other author (1 is '100% confidence'). This suggests that this method, particularly when applied to purely textual input does not have the statistical power to distinguish between Ovidian imitation (_Consolatio_) and genuine work (_Ibis_, _Medicamina_). This leaves the situation unclear with respect to the _Nux_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_verifier = Order2Verifier(\n",
    "    metric=\"minmax\", base=\"instance\", nb_bootstrap_iter=1000, rnd_prop=0.35\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/12/2023 02:35:06 [ruzicka:INFO] Fitting on 470 documents in instance mode...\n"
     ]
    }
   ],
   "source": [
    "real_verifier.fit(vec_ngrams_std.fit_transform(test_corpus.Chunk), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Work</th>\n",
       "      <th>Poem</th>\n",
       "      <th>LEN</th>\n",
       "      <th>Chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Nux</td>\n",
       "      <td>Nux</td>\n",
       "      <td>182</td>\n",
       "      <td>nuks ego junkta wiae kum sim sine krimine wita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Medicamina</td>\n",
       "      <td>Medicamina</td>\n",
       "      <td>100</td>\n",
       "      <td>diskite kwae fakiem kommendet kura puellae\\net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Consolatio</td>\n",
       "      <td>Consolatio 1</td>\n",
       "      <td>158</td>\n",
       "      <td>wisa diu feliks mater modo dikta neronum\\njam ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Consolatio</td>\n",
       "      <td>Consolatio 2</td>\n",
       "      <td>158</td>\n",
       "      <td>at_kwutinam drusi manus alte_ret altera fratri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Consolatio</td>\n",
       "      <td>Consolatio 3</td>\n",
       "      <td>158</td>\n",
       "      <td>kwo raperis laniata komas similiskwe furenti\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Ibis</td>\n",
       "      <td>Ibis 1</td>\n",
       "      <td>64</td>\n",
       "      <td>tempus ad hok lustris bis jam mihi kwinkwe per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Ibis</td>\n",
       "      <td>Ibis 2</td>\n",
       "      <td>200</td>\n",
       "      <td>di maris et terrae kwi_kwis meliora tenetis\\ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Ibis</td>\n",
       "      <td>Ibis 3</td>\n",
       "      <td>200</td>\n",
       "      <td>kwi_kwokulis karuit per kwos male widerat auru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>ps-Ovid</td>\n",
       "      <td>Ibis</td>\n",
       "      <td>Ibis 4</td>\n",
       "      <td>178</td>\n",
       "      <td>aut te dewoweat kertis abdera diebus\\nsaksakwe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Author        Work          Poem  LEN  \\\n",
       "278  ps-Ovid         Nux           Nux  182   \n",
       "279  ps-Ovid  Medicamina    Medicamina  100   \n",
       "280  ps-Ovid  Consolatio  Consolatio 1  158   \n",
       "281  ps-Ovid  Consolatio  Consolatio 2  158   \n",
       "282  ps-Ovid  Consolatio  Consolatio 3  158   \n",
       "283  ps-Ovid        Ibis        Ibis 1   64   \n",
       "284  ps-Ovid        Ibis        Ibis 2  200   \n",
       "285  ps-Ovid        Ibis        Ibis 3  200   \n",
       "286  ps-Ovid        Ibis        Ibis 4  178   \n",
       "\n",
       "                                                 Chunk  \n",
       "278  nuks ego junkta wiae kum sim sine krimine wita...  \n",
       "279  diskite kwae fakiem kommendet kura puellae\\net...  \n",
       "280  wisa diu feliks mater modo dikta neronum\\njam ...  \n",
       "281  at_kwutinam drusi manus alte_ret altera fratri...  \n",
       "282  kwo raperis laniata komas similiskwe furenti\\n...  \n",
       "283  tempus ad hok lustris bis jam mihi kwinkwe per...  \n",
       "284  di maris et terrae kwi_kwis meliora tenetis\\ni...  \n",
       "285  kwi_kwokulis karuit per kwos male widerat auru...  \n",
       "286  aut te dewoweat kertis abdera diebus\\nsaksakwe...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems = elegy_corpus[elegy_corpus.Author == \"ps-Ovid\"]\n",
    "problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/12/2023 02:35:06 [ruzicka:INFO] Predicting on 9 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_verifier.predict_proba(\n",
    "    vec_ngrams_std.transform(problems.Chunk),\n",
    "    np.array(lenc.transform([\"Ovid\"] * len(problems))),\n",
    "    nb_imposters=60,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "208657b13d9d01f546253097cf6f870938d682edd7d0d269d6436fd16db9d0e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
